{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traci_backend import TraciBackend\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, discount, alpha, epsilon):\n",
    "        self.tb = TraciBackend()\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.weights1 = [1] * 4 # for action1\n",
    "        self.weights2 = [1] * 4 # for action2\n",
    "        \n",
    "    def getStateRewardCum(self, edges, num_iter):\n",
    "        '''\n",
    "        Fetch data from traci backend,\n",
    "        return the state and reward\n",
    "        The reward is calculated using data from accumulative step.\n",
    "        '''\n",
    "        tb = self.tb\n",
    "        reward = 0\n",
    "        evaluation = 0\n",
    "        for _ in range(num_iter):\n",
    "            tb.simulate_step()\n",
    "            evaluation += self.getEvaluationUnit()\n",
    "            state = [0] * 4\n",
    "            for n, edge in enumerate(edges):\n",
    "                line_length, total_vehicle_num = \\\n",
    "                    tb.get_halt_vehicle_cnt_edge(edge), tb.get_vehicle_cnt_edge(edge)\n",
    "                pass_vehicle_num = total_vehicle_num - line_length\n",
    "                if n < 2:\n",
    "                    state[0] += line_length\n",
    "                    state[1] += pass_vehicle_num\n",
    "                else:\n",
    "                    state[2] += line_length\n",
    "                    state[3] += pass_vehicle_num\n",
    "                reward += pass_vehicle_num - line_length\n",
    "        return state, reward, tb.is_end(), evaluation\n",
    "        \n",
    "    def getStateReward(self, edges):\n",
    "        '''\n",
    "        Fetch data from traci backend,\n",
    "        return the state and reward of this step\n",
    "        The reward is calculated using data from a specific step,\n",
    "        not accumulated.\n",
    "        '''\n",
    "        tb = self.tb\n",
    "        state, reward = [0] * 4, 0\n",
    "        for n, edge in enumerate(edges):\n",
    "            line_length, total_vehicle_num = tb.get_halt_vehicle_cnt_edge(edge), tb.get_vehicle_cnt_edge(edge)\n",
    "            \n",
    "            pass_vehicle_num = total_vehicle_num - line_length\n",
    "            if n < 2:\n",
    "                state[0] += line_length\n",
    "                state[1] += pass_vehicle_num\n",
    "            else:\n",
    "                state[2] += line_length\n",
    "                state[3] += pass_vehicle_num\n",
    "            reward += pass_vehicle_num - line_length\n",
    "        return state, reward, tb.is_end()\n",
    "    \n",
    "    def getQ(self, state):\n",
    "        '''\n",
    "        Use the function approximator, give out the approximated value\n",
    "        '''\n",
    "        q1, q2 = 0, 0\n",
    "        for i in range(4):\n",
    "            q1 += state[i] * self.weights1[i]\n",
    "            q2 += state[i] * self.weights2[i]\n",
    "        return q1, q2\n",
    "        \n",
    "    def eGreedy(self, q):\n",
    "        '''\n",
    "        Return the action selection 0 or 1 according to q-value\n",
    "        '''\n",
    "        q1, q2 = q[0], q[1]\n",
    "        rand = random.random()\n",
    "        if q1 == q2:\n",
    "            return 0 if rand < 0.5 else 1\n",
    "        if rand < self.epsilon and q1 < q2 or rand > self.epsilon and q1 > q2:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "        \n",
    "    def updateWeight(self, curr_state, next_state, reward, action):\n",
    "        '''\n",
    "        Use approximate Q-learning, update weight in the approximator\n",
    "        '''\n",
    "        q_curr = self.getQ(curr_state)[action] # current approximated q value\n",
    "        q_next_act = self.getQ(next_state) # next approximated q value for both action\n",
    "        next_act = self.eGreedy(q_next_act) # get the action by epsilon greedy\n",
    "        q_next = max(q_next_act)\n",
    "        delta = reward + self.discount * q_next - q_curr\n",
    "        # only update one weights array, either for action1 or action2\n",
    "        if action:\n",
    "            self.weights2 = [self.weights2[i] + self.alpha * delta * curr_state[i] for i in range(4)]\n",
    "        else:\n",
    "            self.weights1 = [self.weights1[i] + self.alpha * delta * curr_state[i] for i in range(4)]\n",
    "        return next_act, action ^ next_act\n",
    "    \n",
    "    def executeAction(self, action, change):\n",
    "        '''\n",
    "        Adjust the lights in sumo given the action 0 or 1\n",
    "        '''\n",
    "        tb = self.tb\n",
    "        evaluation = 0\n",
    "        if not change and action:\n",
    "            tb.set_light_phase('0', 4)\n",
    "            return evaluation\n",
    "        elif not change and not action:\n",
    "            tb.set_light_phase('0', 0)\n",
    "            return evaluation\n",
    "        elif change and action:\n",
    "            tb.set_light_phase('0', 1)\n",
    "            for _ in range(3):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 2)\n",
    "            for _ in range(5):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 3)\n",
    "            for _ in range(3):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 4)\n",
    "        else:\n",
    "            tb.set_light_phase('0', 5)\n",
    "            for _ in range(3):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 6)\n",
    "            for _ in range(5):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 7)\n",
    "            for _ in range(3):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 0)\n",
    "        return evaluation\n",
    "        \n",
    "    def getEvaluationUnit(self):\n",
    "        tb = self.tb\n",
    "        evaluation = 0\n",
    "        for edge in ['1si', '2si', '3si', '4si']:\n",
    "            evaluation += tb.get_halt_vehicle_cnt_edge(edge)\n",
    "        return evaluation\n",
    "    \n",
    "    def train(self, step_size):\n",
    "        '''\n",
    "        Train the agent with num_iter, observe the reward from traci backend every step_size period.\n",
    "        step_size is the time before observing the state and reward, should be smaller than 31.\n",
    "        '''\n",
    "        tb = self.tb\n",
    "        tb.start()\n",
    "        prev_state = [0] * 4\n",
    "        next_act, change = 0, True\n",
    "        weights_history = [[], []]\n",
    "        isEnd = False\n",
    "        evaluation = 0\n",
    "        while not isEnd:\n",
    "            evaluation += self.executeAction(next_act, change)\n",
    "            for _ in range(step_size):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            state, reward, isEnd = self.getStateReward(['1si', '2si', '3si', '4si'])\n",
    "            weights_history[0].append(self.weights1)\n",
    "            weights_history[1].append(self.weights2)\n",
    "            next_act, change = self.updateWeight(prev_state, state, reward, next_act)\n",
    "            prev_state = state\n",
    "        tb.close()\n",
    "        return weights_history, evaluation\n",
    "    \n",
    "    def train2(self, step_size):\n",
    "        '''\n",
    "        Train the agent with num_iter, observe the reward from traci backend every step_size period.\n",
    "        step_size is the time before observing the state and reward\n",
    "        '''\n",
    "        tb = self.tb\n",
    "        tb.start()\n",
    "        prev_state = [0] * 4\n",
    "        next_act, change = 0, True\n",
    "        weights_history = [[], []]\n",
    "        isEnd = False\n",
    "        evaluation = 0\n",
    "        while not isEnd:\n",
    "            evaluation += self.executeAction(next_act, change)\n",
    "                \n",
    "            state, reward, isEnd, evaluationUnit = self.getStateRewardCum(['1si', '2si', '3si', '4si'], step_size)\n",
    "            weights_history[0].append(self.weights1)\n",
    "            weights_history[1].append(self.weights2)\n",
    "            evaluation += evaluationUnit\n",
    "            next_act, change = self.updateWeight(prev_state, state, reward, next_act)\n",
    "            prev_state = state\n",
    "            \n",
    "        tb.close()\n",
    "        return weights_history, evaluation\n",
    "    \n",
    "        \n",
    "\n",
    "class StaticAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tb = TraciBackend()\n",
    "    def train(self):\n",
    "        evaluation, step = 0, 0\n",
    "        tb = self.tb\n",
    "        tb.start()\n",
    "        while not tb.is_end():\n",
    "            tb.simulate_step()\n",
    "            step += 1\n",
    "            for edge in ['1si', '2si', '3si', '4si']:\n",
    "                evaluation += tb.get_halt_vehicle_cnt_edge(edge)\n",
    "        tb.close()\n",
    "        return evaluation\n",
    "            \n",
    "\n",
    "class LongestQueueFirstAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tb = TraciBackend()\n",
    "    \n",
    "    def getEvaluationUnit(self):\n",
    "        tb = self.tb\n",
    "        evaluation = 0\n",
    "        for edge in ['1si', '2si', '3si', '4si']:\n",
    "            evaluation += tb.get_halt_vehicle_cnt_edge(edge)\n",
    "        return evaluation\n",
    "    \n",
    "    def executeAction(self, action, change):\n",
    "        '''\n",
    "        Adjust the lights in sumo given the action 0 or 1\n",
    "        '''\n",
    "        tb = self.tb\n",
    "        evaluation = 0\n",
    "        if not change and action:\n",
    "            tb.set_light_phase('0', 4)\n",
    "            return evaluation\n",
    "        elif not change and not action:\n",
    "            tb.set_light_phase('0', 0)\n",
    "            return evaluation\n",
    "        elif change and action:\n",
    "            tb.set_light_phase('0', 1)\n",
    "            for _ in range(3):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 2)\n",
    "            for _ in range(5):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 3)\n",
    "            for _ in range(3):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 4)\n",
    "        else:\n",
    "            tb.set_light_phase('0', 5)\n",
    "            for _ in range(3):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 6)\n",
    "            for _ in range(5):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 7)\n",
    "            for _ in range(3):\n",
    "                tb.simulate_step()\n",
    "                evaluation += self.getEvaluationUnit()\n",
    "            tb.set_light_phase('0', 0)\n",
    "        return evaluation\n",
    "    \n",
    "    def train(self, step_size):\n",
    "        evaluation, isEnd, action, change, step = 0, False, 0, False, 0\n",
    "        tb = self.tb\n",
    "        tb.start()\n",
    "        while not tb.is_end():\n",
    "            step += 1\n",
    "            evaluation += self.executeAction(action, change)\n",
    "            for i in range(step_size): \n",
    "                tb.simulate_step()\n",
    "                total_vehicle_num, halting_nums = 0, [0, 0]\n",
    "                for edge in ['1si', '2si', '3si', '4si']:\n",
    "                    halting_nums[edge == '3si' or edge == '4si'] += tb.get_halt_vehicle_cnt_edge(edge)\n",
    "                    total_vehicle_num += tb.get_vehicle_cnt_edge(edge)\n",
    "                evaluation += sum(halting_nums)\n",
    "            next_act = 1 if halting_nums[0] > halting_nums[1] else 0\n",
    "            change = action ^ next_act\n",
    "            action = next_act\n",
    "        tb.close()\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Agent1(gamma, alpha, e):   \n",
    "    simulateAgent = Agent(gamma, alpha, e)\n",
    "    evaluation = []\n",
    "    for _ in range(100):\n",
    "        _, evaluateUnit = simulateAgent.train(20)\n",
    "        evaluation.append(evaluateUnit)\n",
    "    return evaluation\n",
    "\n",
    "def test_Agent2(gamma, alpha, e):\n",
    "    simulateAgent = Agent(gamma, alpha, e)\n",
    "    evaluation = []\n",
    "    for _ in range(100):\n",
    "        _, evaluateUnit = simulateAgent.train2(20)\n",
    "        evaluation.append(evaluateUnit)\n",
    "    return evaluation\n",
    "\n",
    "def test_StaticAgent():\n",
    "    simulateAgent = StaticAgent()\n",
    "    evaluation = []\n",
    "    for _ in range(100):\n",
    "        evaluateUnit = simulateAgent.train()\n",
    "        evaluation.append(evaluateUnit)\n",
    "    return evaluation\n",
    "\n",
    "def test_LQFAgent():\n",
    "    simulateAgent = LongestQueueFirstAgent()\n",
    "    evaluation = []\n",
    "    for _ in range(100):\n",
    "        evaluateUnit = simulateAgent.train(20)\n",
    "        evaluation.append(evaluateUnit)\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation1 = test_Agent1(0.9, 0.001, 0.05)\n",
    "evaluation2 = test_Agent2(0.2, 0.001, 0.05)\n",
    "evaluation3 = test_StaticAgent()\n",
    "evaluation4 = test_LQFAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation6_0 = test_Agent1(0.9, 0.001, 0.05)\n",
    "evaluation7 = test_Agent1(0.8, 0.001, 0.05)\n",
    "evaluation8 = test_Agent1(0.8, 0.0003, 0.05)\n",
    "evaluation9 = test_Agent1(0.3, 0.001, 0.05)\n",
    "evaluation6_1 = test_Agent1(0.9, 0.001, 0.05)\n",
    "evaluation6_2 = test_Agent1(0.9, 0.001, 0.05)\n",
    "evaluation6_3 = test_Agent1(0.9, 0.001, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(evaluation6_0)\n",
    "plt.show()\n",
    "plt.plot(evaluation6_1)\n",
    "plt.show()\n",
    "plt.plot(evaluation6_2)\n",
    "plt.show()\n",
    "plt.plot(evaluation6_3)\n",
    "plt.show()\n",
    "plt.plot(evaluation7)\n",
    "plt.show()\n",
    "plt.plot(evaluation8)\n",
    "plt.show()\n",
    "plt.plot(evaluation9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_best = ','.join([str(num) for num in evaluation8])\n",
    "text_ok = ','.join([str(num) for num in evaluation6_2])\n",
    "with open(\"QLearning_best.txt\", \"w\") as f:\n",
    "    f.write(text_best)\n",
    "with open(\"QLearning_ok.txt\", \"w\") as f:\n",
    "    f.write(text_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_static = ','.join([str(num) for num in evaluation3])\n",
    "with open(\"Static.txt\", \"w\") as f:\n",
    "    f.write(text_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(evaluation1)\n",
    "plt.show()\n",
    "plt.plot(evaluation2)\n",
    "plt.show()\n",
    "plt.plot(evaluation3)\n",
    "plt.show()\n",
    "plt.plot(evaluation4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
